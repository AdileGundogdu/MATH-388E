{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5\n",
    "\n",
    "For this homework, we are going to work with [*Indoor User Movement Prediction from RSS data*](https://archive.ics.uci.edu/ml/datasets/Indoor+User+Movement+Prediction+from+RSS+data) dataset from UCI.  The homework is due Friday, December 21st midnight. \n",
    "\n",
    "## Task 1\n",
    "\n",
    "Download the dataset and unzip it in under a subdirectory of `data` folder named `rss_data`.\n",
    "\n",
    "The files we are interested is in the subfolder `dataset`.  Each of these files whose names that start with `MovementAAL_RSS_` contain data collected from indivuduals. Each of these files represent a single data point.  There are 314 of these files, and hence, you have 314 data points.  Each file has 4 columns but the number of rows change from file to file.  \n",
    "\n",
    "There is also a file named `MovementALL_target.csv` in that folder. This file tells us the class each of these measurements are assigned. Some of these measurements are labelled with +1 and some are labelled with -1.\n",
    "\n",
    "## Task 2\n",
    "\n",
    "Construct a SVM model that separates +1 labelled data points from -1 data points.  You must first solve the problem that these datapoints do not have the same number of rows even though they all have the same number of columns. \n",
    "\n",
    "## Task 3\n",
    "\n",
    "Using [Keras](https://keras.io/getting-started/sequential-model-guide/) write a neural network model that separates +1 labelled data points from -1 data points.\n",
    "\n",
    "## Notes\n",
    "\n",
    "1. You must document each step of your tasks: what are you doing, why are you doing it, what problems you encountered and how you solved it.  All of these must be explained and documented.  Solutions without sufficient documentations will be penalized accordingly. 50% of your points will come from your code, while the other 50% will come from your explanations.\n",
    "\n",
    "1. You can use MS Excel to inspect the files, but loading them up to python using pandas and inspecting them there under jupyter is easier.\n",
    "\n",
    "3. Put the data in a separate subfolder of your `data` folder and rename it `rss_data`. I'll take points off if the data is not saved under the correct place.\n",
    "\n",
    "1. For both of Task 2 and Task 3, you must split your data into a train and test set, and then evaluate the accuracy of your model on the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 : SVM\n",
    "\n",
    "Veri kümesini indirdim ve sonrasında inceledim. -1 yazıyorsa değiştirmedi demek 1 diyorsa değiştirdi demek. Sonrasında bu kümeyi test ve train kümelerine ayırmam gerekti. Tabi ki başlangıçta nasıl ayıracağımı bulamadım. Sonrasında danıştığım insanlar sayesinde aşağıdaki yolu buldum. Öncelikle import edilmesi gereken şeyleri edelim.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from numpy import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi de verileri ekleyelim.Birden fazla verimiz olduğu için hepsini tek tek almıycaz bunun için bir fonksiyon oluşturup onunla hepsini birden alacağız. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_target = pd.read_csv('../data/rss_data/dataset/MovementAAL_target.csv',\n",
    "                                   names=('sequence_ID','class_label'),\n",
    "                                   skiprows=(1)) # ds = dataset ve skiprows diyoruz çünkü ilk satır yazı biz sayıları alıyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(identity):\n",
    "    return pd.read_csv('../data/rss_data/dataset/MovementAAL_RSS_%s.csv' %identity,\n",
    "                                  names=('RSS_anchor1','RSS_anchor2','RSS_anchor3','RSS_anchor4'),\n",
    "                                  skiprows=(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aldığımız verileri birleştirmemiz gerekiyor çünkü data setimiz parça parça şeklinde. Onları vektör gibi düşünüp birleştirceğiz. Uzunlukları eşit olmadığı için de ayrıcadan işlem yapacağız. Oluşturduktan sonra test ve train şeklinde bölmeye başlayabiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create(ds):\n",
    "    sets  = []\n",
    "    target = []\n",
    "    for identity_x, row in ds.iterrows():\n",
    "        data = get_dataset(row['sequence_ID'])\n",
    "        sets.append(data)\n",
    "        array = [row['class_label']]*len(data)\n",
    "        target += array \n",
    "        \n",
    "    return pd.concat(sets), target\n",
    "\n",
    "data_test, target_test = create(ds_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boş olan dataları 0 ile doldurmuş olduk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_test\n",
    "y = target_test\n",
    "\n",
    "\n",
    "x_train = x[ : int(random.uniform(0,13196))]\n",
    "y_train = y[ : int(random.uniform(1319,13197))]\n",
    "x_test = x[ :  int(random.uniform(0,13196))]\n",
    "y_test = y[ : int(random.uniform(13197,13197))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Burada ayırdığımız veriyi aşağıdada da kullanmamız gerekecek Keras için de lazım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 841  538]\n",
      " [ 335 1586]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7354545454545455"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification =SVC(kernel = 'rbf' , gamma = 5.0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25)\n",
    "\n",
    "classification.fit(x_train, y_train)\n",
    "predicted = classification.predict(x_test)\n",
    "\n",
    "print(confusion_matrix(y_test, predicted))\n",
    "accuracy_score(y_test,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c (gamma ya da aldığımız kernela bağlı olan parametre) değişince değişmiyor ancak kernel değişince değişiyor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 : Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adile\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , LSTM , Dropout , Conv1D\n",
    "from keras.optimizers import Adamax , RMSprop , SGD \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yukardakiler Keras kütüphanesini çalıştırabilmemiz için import etmemiz gerekenler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9897/9897 [==============================] - 1s 92us/step - loss: 0.8106\n",
      "Epoch 2/10\n",
      "9897/9897 [==============================] - 1s 61us/step - loss: 0.7657\n",
      "Epoch 3/10\n",
      "9897/9897 [==============================] - 1s 57us/step - loss: 0.7542\n",
      "Epoch 4/10\n",
      "9897/9897 [==============================] - 1s 59us/step - loss: 0.7494\n",
      "Epoch 5/10\n",
      "9897/9897 [==============================] - 1s 56us/step - loss: 0.7457\n",
      "Epoch 6/10\n",
      "9897/9897 [==============================] - 1s 62us/step - loss: 0.7413\n",
      "Epoch 7/10\n",
      "9897/9897 [==============================] - 1s 64us/step - loss: 0.7388\n",
      "Epoch 8/10\n",
      "9897/9897 [==============================] - 1s 60us/step - loss: 0.7354\n",
      "Epoch 9/10\n",
      "9897/9897 [==============================] - 1s 60us/step - loss: 0.7324\n",
      "Epoch 10/10\n",
      "9897/9897 [==============================] - 1s 61us/step - loss: 0.7313\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1db06763ac8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25)\n",
    "\n",
    "\n",
    "model.add(Dense(100, input_shape = (4,), activation='relu'))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(30, activation = 'relu'))\n",
    "model.add(Dropout(0.01))\n",
    "model.add(Dense(1, activation = 'relu'))\n",
    "\n",
    "\n",
    "model.compile(optimizer=Adamax(lr=0.004), loss='mean_absolute_error')\n",
    "model.fit(np.array(x_train), np.array(y_train) , batch_size=20 , epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sürekli artmasını beklerdim ancak azalması beni şaşırttı demek ki bu veri kümesinde bu hatayı veriyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
